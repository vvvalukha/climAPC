{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f740b784-fdd6-49e3-b6cd-8708d97fb78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy import interpolate\n",
    "from scipy.interpolate import interp2d\n",
    "from mpl_toolkits.basemap import Basemap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680abef1-93df-4adc-a604-356bcf7a720d",
   "metadata": {},
   "source": [
    "Initial data: gridded SSHA data from CMEMS, available at https://data.marine.copernicus.eu/product/SEALEVEL_GLO_PHY_L4_MY_008_047/description .  \n",
    "“Generated using E.U. Copernicus Marine Service Information; <https://doi.org/10.48670/moi-00148>”\n",
    "\n",
    "For visualisation purposes, I took out daily SSHA files, relevant for April 2014.\n",
    "The script performs following procedures:\n",
    "1. Renames original data according to respective days, sorts them and stores in a user-defined directory 'output_dir'\n",
    "2. Outliers are removed, using the percentile method - the outlliers are prescribed the values of respective thresholds\n",
    "3. NaNs are saved as a mask and converted to 0 in the original array\n",
    "4. The array is subjected to 2D cubic interpolation (smoothes outliers set to threshold values, deals with NaNs that we converted to zeroes)\n",
    "6. Then the interpolation is assessed on a finer grid (0.25x0.25deg. original resolution vs 0.1x0.1deg. new resolution)\n",
    "7. Raw and interpolated data are plotted on basemaps, Drake passage region is chosen as an example\n",
    "8. Maps are saved in .jpg format in 'map_dir'\n",
    "9. Maps are combined in .gif and saved in 'output_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce0db2f3-ddf3-4c89-9f17-85c628bd1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:/Users/vavolk001/Desktop/Thesis/data/CMEMS/SSH_gridded/2014/04' # the directory where original .nc files with SSHA data are stored. Unzip the data, place it wherever is suitable, change the directory up to the last folder\n",
    "map_dir = 'C:/Users/vavolk001/Desktop/climAPC/maps/' # the directory where plotted SSHA maps are stored\n",
    "output_dir = 'C:/Users/vavolk001/Desktop/climAPC/' # the directory where renamed sorted .nc data and final .gif is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc048737-ac90-47fb-b048-ee8952d2ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_fontsize = 66\n",
    "label_fontsize = 52\n",
    "colorbar_fontsize = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3684640e-b2e9-4611-af0f-98b6523bbc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of netCDF files in the data directory\n",
    "file_list = [f for f in os.listdir(data_dir) if f.endswith('.nc')]\n",
    "# Sort the file list alphabetically\n",
    "file_list.sort()\n",
    "\n",
    "# Loop through each file and rename it\n",
    "for idx, file_name in enumerate(file_list):\n",
    "    # Extract the day number from the index\n",
    "    day_number = str(idx + 1).zfill(2)  # Convert index to 2-digit string\n",
    "    # Construct the new file name\n",
    "    new_file_name = day_number + '_04_2014.nc'\n",
    "    # Create the full file paths\n",
    "    old_file_path = os.path.join(data_dir, file_name)\n",
    "    new_file_path = os.path.join(output_dir, new_file_name)\n",
    "    # Rename the file\n",
    "    shutil.copy(old_file_path, new_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2e3c5a6-6a8a-469b-b081-d9c6ea8e8bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vavolk001\\Anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "# Get a list of netCDF files in the output directory\n",
    "file_list = [f for f in os.listdir(output_dir) if f.endswith('.nc')]\n",
    "# Sort the file list alphabetically\n",
    "file_list.sort()\n",
    "\n",
    "for file_name in file_list:\n",
    "    fig,ax = plt.subplots(2,sharex=True,figsize=(102, 112))\n",
    "    gs = gridspec.GridSpec(2, 2, height_ratios=[3, 1])\n",
    "    fig.suptitle('Raw vs Processed vs Processed Interpolated SSHA')\n",
    "    contour_levels = np.linspace(-0.5,0.5, 20)\n",
    "    cmap = 'seismic'\n",
    "    parallels = np.arange(80.,-81,-20.)\n",
    "    #labels = [left,right,top,bottom]\n",
    "    meridians = np.arange(10.,351.,20.)\n",
    "    \n",
    "    # 0. Loading data\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    data = xr.open_dataset(file_path, engine='netcdf4')\n",
    "    dynamic_topography = data['sla'].values\n",
    "    dyn_topo_raw = np.squeeze(dynamic_topography, axis=0)\n",
    "    latitude_raw = data['latitude'].values\n",
    "    longitude_raw = data['longitude'].values\n",
    " \n",
    "    # 1. Removing outliers\n",
    "    # Define the lower and upper percentile thresholds\n",
    "    low_perc = 5  # lower percentile threshold (e.g., 5%)\n",
    "    up_perc = 95  # upper percentile threshold (e.g., 95%)\n",
    "    # Calculate the percentiles along the latitude axis (axis=0)\n",
    "    low_thresh = np.max(np.nanpercentile(dyn_topo_raw, low_perc))\n",
    "    up_thresh = np.min(np.nanpercentile(dyn_topo_raw, up_perc))\n",
    "    # Remove outliers based on the percentile thresholds\n",
    "    dyn_topo_filt = np.where((dyn_topo_raw < low_thresh), low_thresh, dyn_topo_raw)\n",
    "    dyn_topo_filt = np.where((dyn_topo_raw > up_thresh), up_thresh, dyn_topo_raw)\n",
    "    \n",
    "    # 2. Removing missing values\n",
    "    lon, lat = np.meshgrid(longitude_raw, latitude_raw)\n",
    "    mask = np.isnan(dyn_topo_filt) #masking coarse grid\n",
    "    filled_dyn_topo = np.where(mask, 0, dyn_topo_filt) #fill that mask with zeroes\n",
    "    dyn_topo = interp2d(longitude_raw, latitude_raw, filled_dyn_topo, kind='cubic') #filled interpolation, coarse grid\n",
    "    dt_interp = dyn_topo(longitude_raw, latitude_raw) #we get that interpolation, coarse grid\n",
    "    \n",
    "    # 3. Create a new, finer 0.1x0.1 grid\n",
    "    targ_res = 0.1 # target resolution\n",
    "    targ_lon = np.arange(longitude_raw[0], longitude_raw[-1] , targ_res)\n",
    "    targ_lat = np.arange(latitude_raw[0], latitude_raw[-1] , targ_res)\n",
    "    \n",
    "    # 4. Interpolate on a finer grid\n",
    "    fine_interp = dyn_topo(targ_lon, targ_lat)\n",
    "    \n",
    "    # 5. Initiate basemap and corresponding coordinates\n",
    "    m=Basemap(projection='cyl',llcrnrlon=-110, llcrnrlat=-80, urcrnrlon=-10, urcrnrlat=-20)\n",
    "    t_lat, t_lon = m(targ_lat, targ_lon)\n",
    "    latitude,longitude = m(lat,lon)\n",
    "    t_lon,t_lat = np.meshgrid(t_lon,t_lat)\n",
    "    \n",
    "    # 6. Plotting raw vs processed and interpolated data \n",
    "    # Raw data\n",
    "    fig.subplots_adjust(right=0.9)\n",
    "    m0 = Basemap(projection='cyl',llcrnrlon=-110, llcrnrlat=-80, urcrnrlon=-10, urcrnrlat=-20)\n",
    "    m0.ax = ax[0]\n",
    "    m0.drawparallels(parallels,labels=[False,True,False,False], color='0.8')\n",
    "    m0.drawmeridians(meridians,labels=[True,False,False,True], color='0.8')\n",
    "    a=m0.contourf(longitude, latitude, dyn_topo_raw, cmap='seismic', levels=contour_levels, extend='both')\n",
    "    m0.fillcontinents(color='white')\n",
    "    m0.drawcoastlines()\n",
    "\n",
    "    # Processed and interpolated data\n",
    "    m1 = Basemap(projection='cyl',llcrnrlon=-110, llcrnrlat=-80, urcrnrlon=-10, urcrnrlat=-20)\n",
    "    m1.ax = ax[1]\n",
    "    m1.drawparallels(parallels,labels=[False,True,False,False], color='0.8')\n",
    "    m1.drawmeridians(meridians,labels=[True,False,False,True], color='0.8')\n",
    "    b=m1.contourf(t_lon, t_lat, fine_interp, cmap='seismic', levels=contour_levels, extend='both')\n",
    "    m1.drawcoastlines()\n",
    "    m1.fillcontinents(color='white')\n",
    "      \n",
    "    cbar=fig.colorbar(a, ax=ax.ravel().tolist(),aspect=40)\n",
    "    cbar.set_label('Sea surface height anomaly, m', fontsize=label_fontsize)\n",
    "    \n",
    "    ax[1].set_xlabel('Longitude', labelpad=20,fontsize=label_fontsize)\n",
    "    ax[0].set_ylabel('Latitude', labelpad=20,fontsize=label_fontsize)\n",
    "    ax[1].set_ylabel('Latitude', labelpad=20,fontsize=label_fontsize)\n",
    "    \n",
    "    # 7. Saving maps\n",
    "    fig.savefig(os.path.join(map_dir, f'{file_name}.jpg'))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36b3804f-8b7c-4483-872c-477d903ac77d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 8. Saving maps into a gif\n",
    "import os\n",
    "from PIL import Image\n",
    "output_dir = map_dir+'merged_maps.gif'\n",
    "def merge_maps_into_gif(map_dir, output_dir):\n",
    "    # Get a list of all .jpg files in the map directory\n",
    "    file_list = [f for f in os.listdir(map_dir) if f.endswith('.jpg')]\n",
    "    # Sort the file list alphabetically\n",
    "    file_list.sort()\n",
    "\n",
    "    images = []\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(map_dir, file_name)\n",
    "        image = Image.open(file_path)\n",
    "        images.append(image)\n",
    "\n",
    "    # Save the images list as a GIF file\n",
    "    images[0].save(output_dir, format='GIF', save_all=True, append_images=images[1:], duration=0.1, loop=0)\n",
    "\n",
    "# Call the merge_maps_into_gif function\n",
    "merge_maps_into_gif(map_dir, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
